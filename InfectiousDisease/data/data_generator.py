import numpy as npimport tensorflow as tfdef prior(batch_size):    """    Calculate prior distributions using np.random.uniform(lower_bound, upper_bound)    with suitable lower and upper bounds for each parameters. Parameters are: beta, gamma.    :param batch_size: number of samples to draw from the prior    :return: np.ndarray of shape (batch_size, number_parameters) with the samples batch of parameters    """    # parameters = ['beta', 'alpha', 'gamma', 'delta', 'rho']    parameters = ['beta', 'alpha', 'gamma', 'delta', 'eta']    low = [0.8, 0.25, 0.1, 0.01, 0.025]    high = [2.25, 0.75, 1.0, 0.4, 0.45]    low_zip = zip(parameters, low)    high_zip = zip(parameters, high)    low_dict = dict(low_zip)    high_dict = dict(high_zip)    theta = np.random.uniform(low=list(low_dict.values()),                              high=list(high_dict.values()), size=(batch_size, 5))    return thetadef data_model(parameters, t, initial_values):    """    With the initial values, t, and the priors of the parameters;    we can calculate the different values of S, E, I, R, D at different time steps (considering the priors)    We have:    User defined parameters: S_init, I_init    Generate sample from prior p(x|theta)    :param parameters: generator of the data np.ndarray of dimension (number_parameters, )    :param t:    :param initial_values:    :return:    """    S_0, E_0, I_0, D_0, R_0 = initial_values    N = S_0 + E_0 + I_0 + R_0 + D_0    S, E, I, D, R = [S_0], [E_0], [I_0], [D_0], [R_0]    # beta, alpha, gamma, delta, rho = parameters    beta, alpha, gamma, delta, eta = parameters    # Core equations for transitions between compartments    dt = t[1] - t[0]    for _ in t[1:]:        # next_S = S[-1] + (-rho * beta * S[-1] * I[-1]) * dt        # next_E = E[-1] + (rho * beta * S[-1] * I[-1] - alpha * E[-1]) * dt        # next_I = I[-1] + (alpha * E[-1] - gamma * I[-1] - delta * I[-1]) * dt        # next_R = R[-1] + (gamma * I[-1]) * dt        # next_D = D[-1] + (delta * I[-1]) * dt        next_S = S[-1] - ((beta * S[-1] * I[-1]) / N) * dt        next_E = E[-1] + (beta * S[-1] * I[-1] / N - delta * E[-1]) * dt        # next_I = I[-1] + (delta * E[-1] - (1 - alpha) * gamma * I[-1] - alpha * rho * I[-1]) * dt        next_I = I[-1] + (delta * E[-1] - (1 - alpha) * gamma * I[-1] - eta * I[-1]) * dt        # next_I = I[-1] + (delta * E[-1] - (1 - alpha) * gamma * I[-1] - alpha * rho * I[-1]) * dt        next_I = I[-1] + (delta * E[-1] - (1 - alpha) * gamma * I[-1] - eta * I[-1]) * dt        next_R = R[-1] + ((1 - alpha) * gamma * I[-1]) * dt        # next_D = D[-1] + alpha * rho * I[-1] * dt        next_D = D[-1] + eta * I[-1] * dt        S.append(next_S)        E.append(next_E)        I.append(next_I)        R.append(next_R)        D.append(next_D)    return np.stack([S, E, I, R, D]).Tdef data_generator(batch_size, t_max=100, dt=1, N=1000, to_tensor=True):    # Time steps    t = np.linspace(0, t_max, int(t_max/dt))    # Initial values for compartments S, E, I, R, D    initial_values = 1 - 1 / N, 1 / N, 0, 0, 0    # Sample parameters from the prior distributions.    # We sample the parameters from the prior distribution for each instance of the training example    # theta is a np.array of shape (batch_size, n_parameters).    theta = prior(batch_size)    # Generate data    # For each instance of the training set and for each time step, we calculate vectors S, E, I, R, D,    # obtaining a ndarray of dimension (batch_size, time_steps, 5)    x = np.apply_along_axis(func1d=data_model, axis=1, arr=theta, t=t, initial_values=initial_values)    if to_tensor:        theta = tf.convert_to_tensor(theta, dtype=tf.float32)        x = tf.convert_to_tensor(x, dtype=tf.float32)    return {'theta': theta, 'x': x}