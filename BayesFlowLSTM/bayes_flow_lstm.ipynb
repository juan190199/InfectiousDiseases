{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "# Data\n",
    "from data.data_generator import data_generator\n",
    "from data.data_generator import data_model\n",
    "from data.data_filtering import (sanity_checks, est_sanity_checks, sample_sanity_checks)\n",
    "\n",
    "# Model\n",
    "from models.models import HeteroskedasticModel, SequenceNet\n",
    "from models.models import BayesFlow\n",
    "\n",
    "# Utils\n",
    "from utils.losses import heteroskedastic_loss, maximum_likelihood_loss\n",
    "from utils.training import train_step\n",
    "from utils.viz import (plot_true_est_scatter, plot_true_est_posterior, \n",
    "                       plot_correlation_parameters, plot_comp_post_prior, plot_ppc)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Heteroskedastic model as summary net\n",
    "class SummaryNet(tf.keras.Model):\n",
    "    def __init__(self, meta):\n",
    "        super(CustomSummaryNetwork, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the summary network\n",
    "        Arguments:\n",
    "        x : tf.Tensor of shape (batch_size, n_obs, x_dim) - a batch of samples from p(x|params)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Do something with input\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network hyperparameters\n",
    "inv_meta = {\n",
    "    'n_units': [64, 64, 64],\n",
    "    'activation': 'elu',\n",
    "    'w_decay': 0.0,\n",
    "    'initializer': 'glorot_uniform'\n",
    "}\n",
    "n_inv_blocks = 5\n",
    "\n",
    "# Optional if using the predefined summary nets\n",
    "summary_meta = {\n",
    "    'lstm_units': [192, 192, 192],\n",
    "    'activation': 'elu',\n",
    "    'w_decay': 0.0,\n",
    "    'initializer': 'glorot_uniform'\n",
    "}\n",
    "\n",
    "\n",
    "# Forward model hyperparameters\n",
    "parameter_names = [r'$\\beta$', r'$\\alpha$', r'$\\gamma$', r'$\\delta$', r'$\\eta$']\n",
    "theta_dim = len(parameter_names)\n",
    "n_test = 500\n",
    "\n",
    "\n",
    "# Training and optimizer hyperparameters\n",
    "ckpt_file = \"SEIDR_lstm\"\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "iterations_per_epoch = 1000\n",
    "n_samples_posterior = 2000\n",
    "\n",
    "starter_learning_rate = 0.001\n",
    "global_step = tf.Variable(0, dtype=tf.int32)\n",
    "decay_steps = 1000\n",
    "decay_rate = .95\n",
    "learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(starter_learning_rate, decay_steps, decay_rate)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_test = data_generator(n_test)\n",
    "\n",
    "# Preprocessing untrained data\n",
    "x_test = np.array(data_test['x'])\n",
    "theta_test = np.array(data_test['theta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x_test))\n",
    "print(x_test.shape)\n",
    "print(type(theta_test))\n",
    "print(theta_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "x_test, theta_test = sanity_checks(x_test, theta_test)\n",
    "\n",
    "# Sanity checks for numerical stability\n",
    "assert(np.sum(x_test == np.inf) == 0)\n",
    "assert(np.sum(x_test == -np.inf) == 0)\n",
    "assert(np.sum(x_test == np.nan) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-grain",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape)\n",
    "print(theta_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "summary_net = SequenceNet()\n",
    "model = BayesFlow(inv_meta, n_inv_blocks, theta_dim, summary_net=summary_net, permute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_est_scatter(model, x_test, theta_test, n_samples_posterior, parameter_names, figsize=(12, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(step=global_step, optimizer=optimizer, net=model)\n",
    "manager = tf.train.CheckpointManager(checkpoint, './checkpoints/{}'.format(ckpt_file), max_to_keep=3)\n",
    "checkpoint.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ToDo: Disable reshape when training\n",
    "for ep in range(1, epochs + 1):\n",
    "    with tqdm(total=iterations_per_epoch, desc='Training epoch {}'.format(ep)) as p_bar:\n",
    "        losses = train_step(model=model, \n",
    "                            optimizer=optimizer,\n",
    "                            loss_fn=maximum_likelihood_loss, \n",
    "                            iterations=iterations_per_epoch,\n",
    "                            batch_size=batch_size,\n",
    "                            p_bar=p_bar,\n",
    "                            global_step=global_step) \n",
    "\n",
    "        # Manage checkpoint\n",
    "        manager.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
